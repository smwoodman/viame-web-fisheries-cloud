{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"VIAME-Web-NOAA-GCP Documentation The site details how to deploy and manage an instance of VIAME-Web on a NOAA Fisheries-hardened virtual machine (VM) in a GCP Project within the NOAA Fisheries Cloud . These instructions were developed from DIVE Deployment Scenario 1 with the extensive help and expertise of the VIAME team, in particular Brandon Davis and Bryon Lewis, and Ed Rodgers. Note that these docs do not supersede any of the DIVE docs . Rather, they supplement the DIVE deployment options by providing NOAA Fisheries Cloud-specific deployment instructions. Deploy your own instance See the Deployment Guide for step-by-step instructions on how to deploy your own instance of VIAME-Web in GCP in the NOAA Fisheries Cloud. Terminology Per Kitware's concepts and definitions , a deployment of VIAME-Web is a deployment of DIVE Web with associated marine biology-centric pipelines and algorithms . This repo and associated documentation were designed for users deploying the whole VIAME-Web platform, but note that they could also be used to deploy only an instance of DIVE-Web.","title":"Home"},{"location":"#viame-web-noaa-gcp-documentation","text":"The site details how to deploy and manage an instance of VIAME-Web on a NOAA Fisheries-hardened virtual machine (VM) in a GCP Project within the NOAA Fisheries Cloud . These instructions were developed from DIVE Deployment Scenario 1 with the extensive help and expertise of the VIAME team, in particular Brandon Davis and Bryon Lewis, and Ed Rodgers. Note that these docs do not supersede any of the DIVE docs . Rather, they supplement the DIVE deployment options by providing NOAA Fisheries Cloud-specific deployment instructions.","title":"VIAME-Web-NOAA-GCP Documentation"},{"location":"#deploy-your-own-instance","text":"See the Deployment Guide for step-by-step instructions on how to deploy your own instance of VIAME-Web in GCP in the NOAA Fisheries Cloud.","title":"Deploy your own instance"},{"location":"#terminology","text":"Per Kitware's concepts and definitions , a deployment of VIAME-Web is a deployment of DIVE Web with associated marine biology-centric pipelines and algorithms . This repo and associated documentation were designed for users deploying the whole VIAME-Web platform, but note that they could also be used to deploy only an instance of DIVE-Web.","title":"Terminology"},{"location":"admin-general/","text":"General This page provides a summary of recommended best practices for managing your own deployment of VIAME-Web in GCP in the NOAA Fisheries Cloud. Note that these admin options should be evaluated and only used if appropriate to your specific use case. Addon Management To download and scan for VIAME addons, such as canned pipelines and algorithms, see the Addon Management docs . Staying up to date Keeping your deployment up to date is crucial for getting the latest VIAME-Web features and bug fixes, as well as for debugging purposes. It is strongly recommended that you update your images at least once per week. The startup scripts that are created on your VMs during deployment provisioning include docker-compose pull calls that will update the containers installed on your VM(s) to the latest images. Thus, these containers will be updated every time you run the startup script. Depending on your VM management strategy this may be sufficient, particularly if you turn off your VM(s) at least once per week. If your VM (default or web) will be on at all times, you should consider the Production deployment options. Note that keeping images up to date will not update canned VIAME addons. See Addon Management for details on updating VIAME addons. Data backup Data in GCS Storage buckets are quite durable . See the GCS Storage docs for more details about data redundancy, versioning, etc. You can create a GCP Snapshot Schedule to create snapshot backups of your disk(s). See here for a Terraform code template and usage instructions. VM management Your VM schedule, i.e., when your VMs are on (and incurring more costs) and when they are off, will be up to your group. You can use the Google Cloud Pricing Calculator to estimate costs. If you have a split services deployment, an example schedule might be leaving the web VM during the work week, with an Instance Schedule set to turn it off on Friday evening, and turning the worker on and off as needed to run jobs. This would minimize users needing to turn on the web VM (or turning it off while another user is annotating), which also only having the worker VM on when necessary. Recent feature additions allow all users to see the status of the job queue so they know if someone else is currently running a job. See here for a Terraform code template and usage instructions. Server Branding Config For information on how to configure the brand and messaging that appears in various places in the Web UI, see the Server Branding Config docs . Storing imagery Within GCP, storage on Compute Engine disks is much more expensive than storage in GCS buckets . Thus, it is recommended to store your imagery in a GCS bucket and mirror that bucket in your VIAME-Web deployment as a read-only assetstore. See Cloud Storage Integration .","title":"General"},{"location":"admin-general/#general","text":"This page provides a summary of recommended best practices for managing your own deployment of VIAME-Web in GCP in the NOAA Fisheries Cloud. Note that these admin options should be evaluated and only used if appropriate to your specific use case.","title":"General"},{"location":"admin-general/#addon-management","text":"To download and scan for VIAME addons, such as canned pipelines and algorithms, see the Addon Management docs .","title":"Addon Management"},{"location":"admin-general/#staying-up-to-date","text":"Keeping your deployment up to date is crucial for getting the latest VIAME-Web features and bug fixes, as well as for debugging purposes. It is strongly recommended that you update your images at least once per week. The startup scripts that are created on your VMs during deployment provisioning include docker-compose pull calls that will update the containers installed on your VM(s) to the latest images. Thus, these containers will be updated every time you run the startup script. Depending on your VM management strategy this may be sufficient, particularly if you turn off your VM(s) at least once per week. If your VM (default or web) will be on at all times, you should consider the Production deployment options. Note that keeping images up to date will not update canned VIAME addons. See Addon Management for details on updating VIAME addons.","title":"Staying up to date"},{"location":"admin-general/#data-backup","text":"Data in GCS Storage buckets are quite durable . See the GCS Storage docs for more details about data redundancy, versioning, etc. You can create a GCP Snapshot Schedule to create snapshot backups of your disk(s). See here for a Terraform code template and usage instructions.","title":"Data backup"},{"location":"admin-general/#vm-management","text":"Your VM schedule, i.e., when your VMs are on (and incurring more costs) and when they are off, will be up to your group. You can use the Google Cloud Pricing Calculator to estimate costs. If you have a split services deployment, an example schedule might be leaving the web VM during the work week, with an Instance Schedule set to turn it off on Friday evening, and turning the worker on and off as needed to run jobs. This would minimize users needing to turn on the web VM (or turning it off while another user is annotating), which also only having the worker VM on when necessary. Recent feature additions allow all users to see the status of the job queue so they know if someone else is currently running a job. See here for a Terraform code template and usage instructions.","title":"VM management"},{"location":"admin-general/#server-branding-config","text":"For information on how to configure the brand and messaging that appears in various places in the Web UI, see the Server Branding Config docs .","title":"Server Branding Config"},{"location":"admin-general/#storing-imagery","text":"Within GCP, storage on Compute Engine disks is much more expensive than storage in GCS buckets . Thus, it is recommended to store your imagery in a GCS bucket and mirror that bucket in your VIAME-Web deployment as a read-only assetstore. See Cloud Storage Integration .","title":"Storing imagery"},{"location":"admin-server-migration/","text":"Migrating to a new server Within GCP you may need to move to a new project, to resize a disk, or to use a different image. However, these actions require destroying and recreating VMs, and destroying a VM and its associated disk means that all data stored on that VM (e.g., user annotations or trained models) will be lost. This section details how to do this without losing any user data. Default and Web VM The default and web VMs both store user data, and thus care should be taken before destroying these VMs. If these resources do need to be destroyed and recreated, e.g., to move from a Dev to a Prod project, then you can either a) restore a snapshot or b) follow the instructions below to avoid losing any user data. Migrating to a different server should be as easy as copying a few directories from the web server node (web VM). Everything is in /var/lib/docker/volumes (assuming you haven't modified the \"data-root\" in /etc/docker/daemon.json). Run ( docker-compose down ) to stop the service. Put the following directories in the same place on the new server: /var/lib/docker/volumes/dive_addons /var/lib/docker/volumes/dive_girder_assetstore /var/lib/docker/volumes/dive_mongo_db Tell the new server's docker metadata about the new volumes: docker volume create dive_addons docker volume create dive_girder_assetstore docker volume create dive_mongo_db Run docker-compose up on the new server. The docker official docs have steps for migrating volumes, but they are more complicated because they let you move either named or anonymous volumes. The DIVE volumes are named, so they will likely be easier to move using the steps above. Worker VM If using the split services deployment , no user data are stored on the worker VM and thus the worker VM can safely be destroyed and recreated within your GCP project. Follow the worker deployment instructions to deploy and provision the worker. Be sure to provide the correct IP for the web VM. Note that if you do destroy and recreate the worker, you will have to re-download any VIAME addons .","title":"Server Migration"},{"location":"admin-server-migration/#migrating-to-a-new-server","text":"Within GCP you may need to move to a new project, to resize a disk, or to use a different image. However, these actions require destroying and recreating VMs, and destroying a VM and its associated disk means that all data stored on that VM (e.g., user annotations or trained models) will be lost. This section details how to do this without losing any user data.","title":"Migrating to a new server"},{"location":"admin-server-migration/#default-and-web-vm","text":"The default and web VMs both store user data, and thus care should be taken before destroying these VMs. If these resources do need to be destroyed and recreated, e.g., to move from a Dev to a Prod project, then you can either a) restore a snapshot or b) follow the instructions below to avoid losing any user data. Migrating to a different server should be as easy as copying a few directories from the web server node (web VM). Everything is in /var/lib/docker/volumes (assuming you haven't modified the \"data-root\" in /etc/docker/daemon.json). Run ( docker-compose down ) to stop the service. Put the following directories in the same place on the new server: /var/lib/docker/volumes/dive_addons /var/lib/docker/volumes/dive_girder_assetstore /var/lib/docker/volumes/dive_mongo_db Tell the new server's docker metadata about the new volumes: docker volume create dive_addons docker volume create dive_girder_assetstore docker volume create dive_mongo_db Run docker-compose up on the new server. The docker official docs have steps for migrating volumes, but they are more complicated because they let you move either named or anonymous volumes. The DIVE volumes are named, so they will likely be easier to move using the steps above.","title":"Default and Web VM"},{"location":"admin-server-migration/#worker-vm","text":"If using the split services deployment , no user data are stored on the worker VM and thus the worker VM can safely be destroyed and recreated within your GCP project. Follow the worker deployment instructions to deploy and provision the worker. Be sure to provide the correct IP for the web VM. Note that if you do destroy and recreate the worker, you will have to re-download any VIAME addons .","title":"Worker VM"},{"location":"admin-storage/","text":"Cloud Storage Integration This guide details how to store your imagery in a GCS bucket and mirror that that to your VIAME-Web deployment. This will allow all users to see and use (e.g., annotate, run models on) imagery in the bucket, while ensuring that users cannot delete or modify imagery in the bucket. Annotations are stored in a MongoDB database on the VM. Setup This section expands on the Cloud Storage Integration docs . Creating access credentials Follow the Creating access credentials instructions . You should already have created the service account during deployment . You must create an access key for the same service account that is attached to your VM(s). Setting up CORS Confirm that CORS headers are configured for your GCS bucket(s). Create Assetstore Go to http://localhost:8010/girder#assetstores , and click \u2018Create new Amazon S3 assetstore\u2019. Fill in the options as follows: Assetstore name: The name of the assetstore. It is recommended to use the same name as the bucket. S3 bucket name The name of the GCS bucket. Path prefix (optional): Leave this blank if you wish to mount the whole bucket. Access key ID: The access key ID from creating your service account access credentials. Secret access key: The secret key from creating your service account access credentials. Service: Enter the GCP service url: https://storage.googleapis.com . Region: The GCP region that your bucket is in. This will likely be us-east4 . It is recommended that you check 'Read only' so that users cannot edit the bucket through the VIAME-Web deployment. Click the 'Create' button to create the assetstore. Create local mount point To make these data broadly accessible to the users of your deployment, create a \u2018Collection\u2019. Go to \u2018Collections\u2019 tab on the Girder page ( http://localhost:8010/girder#collections ) and click \u2018Create collection\u2019. Make the collection name the same as the bucket. Create a folder within the collection called 'bucket-mount\u2019. Get the \u2018Unique ID\u2019 of the 'bucket-mount' folder by clicking into the folder and clicking the \u2018I\u2019 (info) button. Copy this unique ID. For users to be able to annotate images from the mounted bucket, you must give them edit permissions on the collection. To give a user permission to annotate any images in the collection: Click into the collection, click 'Actions', and select 'Access control'. Add users as editors as appropriate and select 'Also set permissions on all subfolders to match this collection's permissions'. Click 'Save'. You can also specify more granular levels of permissions as appropriate. Import data Now that the mount point has been created, you must 'import' the data from the bucket. Go back to http://localhost:8010/girder#assetstores and click 'Import data' for the desired assetstore. Enter the unique ID of the 'bucket-mount' folder in the 'Destination ID' box. If you wish to import the whole bucket, leave 'Import path' blank. Click 'Begin import' and the service will begin to import the data so that the imagery and folders in the bucket show up in the collection. Assetstore Management See [these docs] (https://kitware.github.io/dive/Deployment-Storage/#s3-and-minio-mirroring) and this question for data mirroring guidelines. Adding or deleting data Whenever you add new folders and/or imagery to the bucket, you must repeat the 'Import data' step. Currently the mount point can only be kept up to date automatically using Pub/Sub notifications if your server has a public static IP address. Also, without Pub/Sub notifications, you must delete folders from the collection using the VIAME-Web user interface, even after deleting the folders from the bucket.","title":"Cloud Storage Integration"},{"location":"admin-storage/#cloud-storage-integration","text":"This guide details how to store your imagery in a GCS bucket and mirror that that to your VIAME-Web deployment. This will allow all users to see and use (e.g., annotate, run models on) imagery in the bucket, while ensuring that users cannot delete or modify imagery in the bucket. Annotations are stored in a MongoDB database on the VM.","title":"Cloud Storage Integration"},{"location":"admin-storage/#setup","text":"This section expands on the Cloud Storage Integration docs .","title":"Setup"},{"location":"admin-storage/#creating-access-credentials","text":"Follow the Creating access credentials instructions . You should already have created the service account during deployment . You must create an access key for the same service account that is attached to your VM(s).","title":"Creating access credentials"},{"location":"admin-storage/#setting-up-cors","text":"Confirm that CORS headers are configured for your GCS bucket(s).","title":"Setting up CORS"},{"location":"admin-storage/#create-assetstore","text":"Go to http://localhost:8010/girder#assetstores , and click \u2018Create new Amazon S3 assetstore\u2019. Fill in the options as follows: Assetstore name: The name of the assetstore. It is recommended to use the same name as the bucket. S3 bucket name The name of the GCS bucket. Path prefix (optional): Leave this blank if you wish to mount the whole bucket. Access key ID: The access key ID from creating your service account access credentials. Secret access key: The secret key from creating your service account access credentials. Service: Enter the GCP service url: https://storage.googleapis.com . Region: The GCP region that your bucket is in. This will likely be us-east4 . It is recommended that you check 'Read only' so that users cannot edit the bucket through the VIAME-Web deployment. Click the 'Create' button to create the assetstore.","title":"Create Assetstore"},{"location":"admin-storage/#create-local-mount-point","text":"To make these data broadly accessible to the users of your deployment, create a \u2018Collection\u2019. Go to \u2018Collections\u2019 tab on the Girder page ( http://localhost:8010/girder#collections ) and click \u2018Create collection\u2019. Make the collection name the same as the bucket. Create a folder within the collection called 'bucket-mount\u2019. Get the \u2018Unique ID\u2019 of the 'bucket-mount' folder by clicking into the folder and clicking the \u2018I\u2019 (info) button. Copy this unique ID. For users to be able to annotate images from the mounted bucket, you must give them edit permissions on the collection. To give a user permission to annotate any images in the collection: Click into the collection, click 'Actions', and select 'Access control'. Add users as editors as appropriate and select 'Also set permissions on all subfolders to match this collection's permissions'. Click 'Save'. You can also specify more granular levels of permissions as appropriate.","title":"Create local mount point"},{"location":"admin-storage/#import-data","text":"Now that the mount point has been created, you must 'import' the data from the bucket. Go back to http://localhost:8010/girder#assetstores and click 'Import data' for the desired assetstore. Enter the unique ID of the 'bucket-mount' folder in the 'Destination ID' box. If you wish to import the whole bucket, leave 'Import path' blank. Click 'Begin import' and the service will begin to import the data so that the imagery and folders in the bucket show up in the collection.","title":"Import data"},{"location":"admin-storage/#assetstore-management","text":"See [these docs] (https://kitware.github.io/dive/Deployment-Storage/#s3-and-minio-mirroring) and this question for data mirroring guidelines.","title":"Assetstore Management"},{"location":"admin-storage/#adding-or-deleting-data","text":"Whenever you add new folders and/or imagery to the bucket, you must repeat the 'Import data' step. Currently the mount point can only be kept up to date automatically using Pub/Sub notifications if your server has a public static IP address. Also, without Pub/Sub notifications, you must delete folders from the collection using the VIAME-Web user interface, even after deleting the folders from the bucket.","title":"Adding or deleting data"},{"location":"deployment-access/","text":"Access Instance of VIAME-Web The services (Docker containers) installed on the VM(s) must be running to access the VIAME-Web instance; it is not enough just to turn on the VMs. Thus, if you have just turned on the VM, you must run the startup script again to start the services. Run Startup Scripts These commands to start the services can be run from Cloud Shell or a local shell with Google Cloud CLI installed (i.e., a Cloud SDK shell). Note that for other users to run the startup script, they must have permission to run docker-compose on the VM. To allow this, you can add users to the docker group. See manage docker as a non-root user for details. Default ZONE=us-east4-c INSTANCE_NAME=viame-web gcloud compute ssh $INSTANCE_NAME --zone=$ZONE --command=\"/opt/noaa/dive_startup_full.sh\" Split Services ZONE=us-east4-c INSTANCE_NAME_WEB=viame-web-web gcloud compute ssh $INSTANCE_NAME_WEB --zone=$ZONE --command=\"/opt/noaa/dive_startup_web.sh\" If you wish to also start the worker services: INSTANCE_NAME_WORKER=viame-web-worker gcloud compute ssh $INSTANCE_NAME_WORKER --zone=$ZONE --command=\"/opt/noaa/dive_startup_worker.sh\" Create SSH Tunnel Now that you have provisioned and restarted the VM(s), you can almost access the VIAME-Web instance. Assuming the stack is running, you need to tunnel the 8010 port through SSH. The following command needs to be run from a Cloud SDK shell on your local workstation. Note that you may need to define variables and/or change the variable format. gcloud compute ssh $INSTANCE_NAME --zone=$ZONE -- -N -L 8010:localhost:8010 If on a Windows, the first time you run this command you likely will get a PuTTY Security Alert popup window asking if you trust the host. Click either 'Accept' (recommended) or 'Connect Once' to be able to access the server. You should now be able to access the VIAME-Web instance (the web service) at http://localhost:8010 , as described in the DIVE docs .","title":"Accessing VIAME-Web"},{"location":"deployment-access/#access-instance-of-viame-web","text":"The services (Docker containers) installed on the VM(s) must be running to access the VIAME-Web instance; it is not enough just to turn on the VMs. Thus, if you have just turned on the VM, you must run the startup script again to start the services.","title":"Access Instance of VIAME-Web"},{"location":"deployment-access/#run-startup-scripts","text":"These commands to start the services can be run from Cloud Shell or a local shell with Google Cloud CLI installed (i.e., a Cloud SDK shell). Note that for other users to run the startup script, they must have permission to run docker-compose on the VM. To allow this, you can add users to the docker group. See manage docker as a non-root user for details.","title":"Run Startup Scripts"},{"location":"deployment-access/#default","text":"ZONE=us-east4-c INSTANCE_NAME=viame-web gcloud compute ssh $INSTANCE_NAME --zone=$ZONE --command=\"/opt/noaa/dive_startup_full.sh\"","title":"Default"},{"location":"deployment-access/#split-services","text":"ZONE=us-east4-c INSTANCE_NAME_WEB=viame-web-web gcloud compute ssh $INSTANCE_NAME_WEB --zone=$ZONE --command=\"/opt/noaa/dive_startup_web.sh\" If you wish to also start the worker services: INSTANCE_NAME_WORKER=viame-web-worker gcloud compute ssh $INSTANCE_NAME_WORKER --zone=$ZONE --command=\"/opt/noaa/dive_startup_worker.sh\"","title":"Split Services"},{"location":"deployment-access/#create-ssh-tunnel","text":"Now that you have provisioned and restarted the VM(s), you can almost access the VIAME-Web instance. Assuming the stack is running, you need to tunnel the 8010 port through SSH. The following command needs to be run from a Cloud SDK shell on your local workstation. Note that you may need to define variables and/or change the variable format. gcloud compute ssh $INSTANCE_NAME --zone=$ZONE -- -N -L 8010:localhost:8010 If on a Windows, the first time you run this command you likely will get a PuTTY Security Alert popup window asking if you trust the host. Click either 'Accept' (recommended) or 'Connect Once' to be able to access the server. You should now be able to access the VIAME-Web instance (the web service) at http://localhost:8010 , as described in the DIVE docs .","title":"Create SSH Tunnel"},{"location":"deployment-default/","text":"Default Deployment NOTE: Be sure to first read the General deployment instructions. These instructions are for a single VM with at least one GPU, meaning all operations (annotation, training, etc.) will happen on this VM. For hybrid options, e.g., a low-cost compute VM paired with a worker VM with GPUs, see Split Services . Create GCP Resources To create a single VM for an instance of VIAME-Web, create a VM with at least one GPU using the 'viame-web-noaa-gcp' module. The source path to this module can either be relative (e.g., '../viame-web-noaa-gcp') or an unprefixed github.com URL (e.g., 'github.com/us-amlr/viame-web-noaa-gcp'). Be sure to provide a non-zero value for gpu_count. See here for a Terraform code template for a VM for a default deployment. Provision GCP VM Once you have created the VM, set variables in your Cloud Shell session that will be used throughout. Then, download the install script to the VM, make it executable, and then run the install script from within the VM. Respond 'no' to the PAM overwrite question. Note that the install script must be run from within the VM, i.e., after ssh'ing into the VM, to effectively respond to the PAM question. Running this install script may take 10-15 minutes. ZONE=us-east4-c INSTANCE_NAME=viame-web REPO_URL=https://raw.githubusercontent.com/us-amlr/viame-web-noaa-gcp/main/scripts gcloud compute ssh $INSTANCE_NAME --zone=$ZONE \\ --command=\"curl -L $REPO_URL/dive_install.sh -o ~/dive_install.sh && chmod +x ~/dive_install.sh\" # ssh into the VM gcloud compute ssh $INSTANCE_NAME --zone=$ZONE # From within the VM, run: ./dive_install.sh exit #to exit the VM, after the script completes Because of permissions changes and installing the NVIDIA drivers, the VM must now be restarted. While you can restart the VM from the console, it is generally easiest to run the following from Cloud Shell to 1) restart the VM and 2) run the startup script to pull updated files and spin up the VIAME-Web stack: gcloud compute instances stop $INSTANCE_NAME --zone=$ZONE && \\ gcloud compute instances start $INSTANCE_NAME --zone=$ZONE gcloud compute ssh $INSTANCE_NAME --zone=$ZONE --command=\"/opt/noaa/dive_startup_full.sh\" Access VIAME-Web deployment See Access VIAME-Web","title":"Default"},{"location":"deployment-default/#default-deployment","text":"NOTE: Be sure to first read the General deployment instructions. These instructions are for a single VM with at least one GPU, meaning all operations (annotation, training, etc.) will happen on this VM. For hybrid options, e.g., a low-cost compute VM paired with a worker VM with GPUs, see Split Services .","title":"Default Deployment"},{"location":"deployment-default/#create-gcp-resources","text":"To create a single VM for an instance of VIAME-Web, create a VM with at least one GPU using the 'viame-web-noaa-gcp' module. The source path to this module can either be relative (e.g., '../viame-web-noaa-gcp') or an unprefixed github.com URL (e.g., 'github.com/us-amlr/viame-web-noaa-gcp'). Be sure to provide a non-zero value for gpu_count. See here for a Terraform code template for a VM for a default deployment.","title":"Create GCP Resources"},{"location":"deployment-default/#provision-gcp-vm","text":"Once you have created the VM, set variables in your Cloud Shell session that will be used throughout. Then, download the install script to the VM, make it executable, and then run the install script from within the VM. Respond 'no' to the PAM overwrite question. Note that the install script must be run from within the VM, i.e., after ssh'ing into the VM, to effectively respond to the PAM question. Running this install script may take 10-15 minutes. ZONE=us-east4-c INSTANCE_NAME=viame-web REPO_URL=https://raw.githubusercontent.com/us-amlr/viame-web-noaa-gcp/main/scripts gcloud compute ssh $INSTANCE_NAME --zone=$ZONE \\ --command=\"curl -L $REPO_URL/dive_install.sh -o ~/dive_install.sh && chmod +x ~/dive_install.sh\" # ssh into the VM gcloud compute ssh $INSTANCE_NAME --zone=$ZONE # From within the VM, run: ./dive_install.sh exit #to exit the VM, after the script completes Because of permissions changes and installing the NVIDIA drivers, the VM must now be restarted. While you can restart the VM from the console, it is generally easiest to run the following from Cloud Shell to 1) restart the VM and 2) run the startup script to pull updated files and spin up the VIAME-Web stack: gcloud compute instances stop $INSTANCE_NAME --zone=$ZONE && \\ gcloud compute instances start $INSTANCE_NAME --zone=$ZONE gcloud compute ssh $INSTANCE_NAME --zone=$ZONE --command=\"/opt/noaa/dive_startup_full.sh\"","title":"Provision GCP VM"},{"location":"deployment-default/#access-viame-web-deployment","text":"See Access VIAME-Web","title":"Access VIAME-Web deployment"},{"location":"deployment-general/","text":"General Overview These instructions assume that you manage the configuration of your GCP project through Terraform, and specifically that run your Terraform commands through Cloud Shell . Note also that users must have GCP Project Owner or Administrator privileges (e.g., roles/owner ). The subsequent pages will walk you through several steps: creating the virtual machine(s) and other resources in GCP, provisioning the VM(s), and accessing your deployment of VIAME-Web. These instructions work and this deployment is (relatively) straightforward because the VIAME team has created Docker containers for the different services that make up VIAME-Web. Thus, after some VM configuration, we can simply download and run these containers to use spin up our own instance of VIAME-Web. Updating the services is also as simple as pulling down updated versions of the containers. The steps on this page apply to both deployment scenarios. Unless otherwise specified, these commands (and commands in the scenario-specific instructions) are expected to be run from Cloud Shell . Before you begin Read the Deployment Options Overview and Cloud Deployment Guide - Before you begin . Ensure that the required network changes have been made for your project. Ensure that the Google Cloud CLI tools are installed and configured on your local workstation. Clone Repository It is recommended to clone the viame-web-noaa-gcp repo in the home directory of your Cloud Shell to 1) use the module and 2) so that relative paths match these instructions. # from your cloud shell git clone https://github.com/us-amlr/viame-web-noaa-gcp.git Create GCP Resources Both scenarios require many of the same resources, including: a GCS bucket in which to store imagery that will be connected to your VIAME-Web deployment, a NMFS-approved image, and a service account with sufficient permissions. See here for more information about the required CORS headers for the bucket. Note that there might be a delay between a VM has been created and/or started, and when you can run an install or startup script. If you get an error, please wait a few minutes and try to run the command again. See here for a Terraform code template for general resources.","title":"General"},{"location":"deployment-general/#general","text":"","title":"General"},{"location":"deployment-general/#overview","text":"These instructions assume that you manage the configuration of your GCP project through Terraform, and specifically that run your Terraform commands through Cloud Shell . Note also that users must have GCP Project Owner or Administrator privileges (e.g., roles/owner ). The subsequent pages will walk you through several steps: creating the virtual machine(s) and other resources in GCP, provisioning the VM(s), and accessing your deployment of VIAME-Web. These instructions work and this deployment is (relatively) straightforward because the VIAME team has created Docker containers for the different services that make up VIAME-Web. Thus, after some VM configuration, we can simply download and run these containers to use spin up our own instance of VIAME-Web. Updating the services is also as simple as pulling down updated versions of the containers. The steps on this page apply to both deployment scenarios. Unless otherwise specified, these commands (and commands in the scenario-specific instructions) are expected to be run from Cloud Shell .","title":"Overview"},{"location":"deployment-general/#before-you-begin","text":"Read the Deployment Options Overview and Cloud Deployment Guide - Before you begin . Ensure that the required network changes have been made for your project. Ensure that the Google Cloud CLI tools are installed and configured on your local workstation.","title":"Before you begin"},{"location":"deployment-general/#clone-repository","text":"It is recommended to clone the viame-web-noaa-gcp repo in the home directory of your Cloud Shell to 1) use the module and 2) so that relative paths match these instructions. # from your cloud shell git clone https://github.com/us-amlr/viame-web-noaa-gcp.git","title":"Clone Repository"},{"location":"deployment-general/#create-gcp-resources","text":"Both scenarios require many of the same resources, including: a GCS bucket in which to store imagery that will be connected to your VIAME-Web deployment, a NMFS-approved image, and a service account with sufficient permissions. See here for more information about the required CORS headers for the bucket. Note that there might be a delay between a VM has been created and/or started, and when you can run an install or startup script. If you get an error, please wait a few minutes and try to run the command again. See here for a Terraform code template for general resources.","title":"Create GCP Resources"},{"location":"deployment-split/","text":"Split Services Deployment NOTE: Be sure to first read the General deployment instructions. These instructions are for splitting VIAME-Web web and worker services across two VMs: a compute-only web VM for annotations, and a worker VM with one or more GPUs that can be turned on as needed to run jobs. This is a cost-effective solution, as the (expensive) worker VM is only turned on as needed. See the DIVE docs for more details. For running VIAME-Web on s single VM, see the Default Deployment . Create GCP Resources To create two VMs for a split services instance of VIAME-Web, create two VMs: a web and a worker. The infrastructure of the web and worker VMs should be identical, except that the web node will have no GPU, should have a slightly larger disk capacity, and needs SSH AllowTcpForwarding to be enabled. Use the 'viame-web-noaa-gcp' module to create these VMs. The source path to this module can either be relative (e.g., '../viame-web-noaa-gcp') or an unprefixed github.com URL (e.g., 'github.com/us-amlr/viame-web-noaa-gcp'). See here for a Terraform code template for the VMs for a split services deployment. Provision GCP VMs First, we set the variables in Cloud Shell that will be used throughout. Note that both install scripts require the internal IP of the web node. ZONE=us-east4-c INSTANCE_NAME_WEB=viame-web-web INSTANCE_NAME_WORKER=viame-web-worker REPO_URL=https://raw.githubusercontent.com/us-amlr/viame-web-noaa-gcp/main/scripts WEB_INTERNAL_IP=$(gcloud compute instances describe $INSTANCE_NAME_WEB --zone=$ZONE --format='get(networkInterfaces[0].networkIP)') Web VM Once the VMs have been created, download the install script to the VM, make it executable, and then run the install script from within the VM. Respond 'no' to the PAM overwrite question. This install script does not install GPU drivers, and thus all commands can be passed to the VM from Cloud Shell. gcloud compute ssh $INSTANCE_NAME_WEB --zone=$ZONE \\ --command=\"curl -L $REPO_URL/dive_install_web.sh -o ~/dive_install_web.sh \\ && chmod +x ~/dive_install_web.sh \\ && ~/dive_install_web.sh $WEB_INTERNAL_IP\" You still need to restart the VM to allow permissions changes to take effect. Then, run the startup script for the web node. gcloud compute instances stop $INSTANCE_NAME_WEB --zone=$ZONE && \\ gcloud compute instances start $INSTANCE_NAME_WEB --zone=$ZONE gcloud compute ssh $INSTANCE_NAME_WEB --zone=$ZONE --command=\"/opt/noaa/dive_startup_web.sh\" Worker VM Next, provision the worker. Download the install script to the VM, make it executable, and then run the install script from within the VM. Respond 'no' to the PAM overwrite question. Running this install script may take 10-15 minutes. For the internal IP of the web node, you can either copy internal IP of the web node and past it into the command, or set the variables from above within the worker VM to be able to use WEB_INTERNAL_IP. gcloud compute ssh $INSTANCE_NAME_WORKER --zone=$ZONE \\ --command=\"curl -L $REPO_URL/dive_install.sh -o ~/dive_install.sh && chmod +x ~/dive_install.sh\" # ssh into the worker VM gcloud compute ssh $INSTANCE_NAME_WORKER --zone=$ZONE # From within the VM, after assigning relevant variables run: WEB_INTERNAL_IP=$(gcloud compute instances describe $INSTANCE_NAME_WEB --zone=$ZONE --format='get(networkInterfaces[0].networkIP)') ~/dive_install.sh -w $WEB_INTERNAL_IP exit #to exit the VM Because of permissions changes and installing the NVIDIA drivers, the VM must now be restarted. Restart the VM and run the startup script to pull updated files and spin up the VIAME-Web stack: gcloud compute instances stop $INSTANCE_NAME_WORKER --zone=$ZONE && \\ gcloud compute instances start $INSTANCE_NAME_WORKER --zone=$ZONE gcloud compute ssh $INSTANCE_NAME_WORKER --zone=$ZONE --command=\"/opt/noaa/dive_startup_worker.sh\" Access VIAME-Web deployment See Access VIAME-Web Web and Worker VM Communication For the split services to be able to work, the web and worker VMs must be able to communicate. You can confirm this either through either the DIVE API (recommended) or the VMs directly. Before testing the connection, be sure that 1) both the web and worker VMs are on and the services have been started (i.e., the startup scripts have been run) and 2) both VMs have the your viame network tag applied. DIVE API Open the swagger UI at http://{server_url}:{server_port}/api/v1 (likely http://localhost:8010/api/v1 ). Under the 'worker' endpoint, issue a GET /worker/status request. The 'Response Body' section should be a long list of successful connection attempts. If the 'Response Body' values are null , then there is a communication issue. Other SSH into the web VM and check that the VM is listening on at least ports 8010 and 5672. Note that you must have root access to run these commands. # check if VM is listening on any ports - should be at least 8010 and 5672 sudo apt install net-tools #install if necessary netstat -plaunt # Get the internal IP of the web VM from the third block in the output ifconfig SSH into the worker VM and check if the VM can make a connection to the web VM on the expected ports. These commands should output a string like Connection to ##.##.##.## 8010 port [tcp/*] succeeded! . If the worker VM cannot make a connection to the web VM, then you will get a 'operation timed out' message. WEB_IP=##.##.##.## nc -v -w3 $WEB_IP 8010 nc -v -w3 $WEB_IP 5672","title":"Split Services"},{"location":"deployment-split/#split-services-deployment","text":"NOTE: Be sure to first read the General deployment instructions. These instructions are for splitting VIAME-Web web and worker services across two VMs: a compute-only web VM for annotations, and a worker VM with one or more GPUs that can be turned on as needed to run jobs. This is a cost-effective solution, as the (expensive) worker VM is only turned on as needed. See the DIVE docs for more details. For running VIAME-Web on s single VM, see the Default Deployment .","title":"Split Services Deployment"},{"location":"deployment-split/#create-gcp-resources","text":"To create two VMs for a split services instance of VIAME-Web, create two VMs: a web and a worker. The infrastructure of the web and worker VMs should be identical, except that the web node will have no GPU, should have a slightly larger disk capacity, and needs SSH AllowTcpForwarding to be enabled. Use the 'viame-web-noaa-gcp' module to create these VMs. The source path to this module can either be relative (e.g., '../viame-web-noaa-gcp') or an unprefixed github.com URL (e.g., 'github.com/us-amlr/viame-web-noaa-gcp'). See here for a Terraform code template for the VMs for a split services deployment.","title":"Create GCP Resources"},{"location":"deployment-split/#provision-gcp-vms","text":"First, we set the variables in Cloud Shell that will be used throughout. Note that both install scripts require the internal IP of the web node. ZONE=us-east4-c INSTANCE_NAME_WEB=viame-web-web INSTANCE_NAME_WORKER=viame-web-worker REPO_URL=https://raw.githubusercontent.com/us-amlr/viame-web-noaa-gcp/main/scripts WEB_INTERNAL_IP=$(gcloud compute instances describe $INSTANCE_NAME_WEB --zone=$ZONE --format='get(networkInterfaces[0].networkIP)')","title":"Provision GCP VMs"},{"location":"deployment-split/#web-vm","text":"Once the VMs have been created, download the install script to the VM, make it executable, and then run the install script from within the VM. Respond 'no' to the PAM overwrite question. This install script does not install GPU drivers, and thus all commands can be passed to the VM from Cloud Shell. gcloud compute ssh $INSTANCE_NAME_WEB --zone=$ZONE \\ --command=\"curl -L $REPO_URL/dive_install_web.sh -o ~/dive_install_web.sh \\ && chmod +x ~/dive_install_web.sh \\ && ~/dive_install_web.sh $WEB_INTERNAL_IP\" You still need to restart the VM to allow permissions changes to take effect. Then, run the startup script for the web node. gcloud compute instances stop $INSTANCE_NAME_WEB --zone=$ZONE && \\ gcloud compute instances start $INSTANCE_NAME_WEB --zone=$ZONE gcloud compute ssh $INSTANCE_NAME_WEB --zone=$ZONE --command=\"/opt/noaa/dive_startup_web.sh\"","title":"Web VM"},{"location":"deployment-split/#worker-vm","text":"Next, provision the worker. Download the install script to the VM, make it executable, and then run the install script from within the VM. Respond 'no' to the PAM overwrite question. Running this install script may take 10-15 minutes. For the internal IP of the web node, you can either copy internal IP of the web node and past it into the command, or set the variables from above within the worker VM to be able to use WEB_INTERNAL_IP. gcloud compute ssh $INSTANCE_NAME_WORKER --zone=$ZONE \\ --command=\"curl -L $REPO_URL/dive_install.sh -o ~/dive_install.sh && chmod +x ~/dive_install.sh\" # ssh into the worker VM gcloud compute ssh $INSTANCE_NAME_WORKER --zone=$ZONE # From within the VM, after assigning relevant variables run: WEB_INTERNAL_IP=$(gcloud compute instances describe $INSTANCE_NAME_WEB --zone=$ZONE --format='get(networkInterfaces[0].networkIP)') ~/dive_install.sh -w $WEB_INTERNAL_IP exit #to exit the VM Because of permissions changes and installing the NVIDIA drivers, the VM must now be restarted. Restart the VM and run the startup script to pull updated files and spin up the VIAME-Web stack: gcloud compute instances stop $INSTANCE_NAME_WORKER --zone=$ZONE && \\ gcloud compute instances start $INSTANCE_NAME_WORKER --zone=$ZONE gcloud compute ssh $INSTANCE_NAME_WORKER --zone=$ZONE --command=\"/opt/noaa/dive_startup_worker.sh\"","title":"Worker VM"},{"location":"deployment-split/#access-viame-web-deployment","text":"See Access VIAME-Web","title":"Access VIAME-Web deployment"},{"location":"deployment-split/#web-and-worker-vm-communication","text":"For the split services to be able to work, the web and worker VMs must be able to communicate. You can confirm this either through either the DIVE API (recommended) or the VMs directly. Before testing the connection, be sure that 1) both the web and worker VMs are on and the services have been started (i.e., the startup scripts have been run) and 2) both VMs have the your viame network tag applied.","title":"Web and Worker VM Communication"},{"location":"deployment-split/#dive-api","text":"Open the swagger UI at http://{server_url}:{server_port}/api/v1 (likely http://localhost:8010/api/v1 ). Under the 'worker' endpoint, issue a GET /worker/status request. The 'Response Body' section should be a long list of successful connection attempts. If the 'Response Body' values are null , then there is a communication issue.","title":"DIVE API"},{"location":"deployment-split/#other","text":"SSH into the web VM and check that the VM is listening on at least ports 8010 and 5672. Note that you must have root access to run these commands. # check if VM is listening on any ports - should be at least 8010 and 5672 sudo apt install net-tools #install if necessary netstat -plaunt # Get the internal IP of the web VM from the third block in the output ifconfig SSH into the worker VM and check if the VM can make a connection to the web VM on the expected ports. These commands should output a string like Connection to ##.##.##.## 8010 port [tcp/*] succeeded! . If the worker VM cannot make a connection to the web VM, then you will get a 'operation timed out' message. WEB_IP=##.##.##.## nc -v -w3 $WEB_IP 8010 nc -v -w3 $WEB_IP 5672","title":"Other"},{"location":"network-changes/","text":"NOAA Fisheries Cloud Network Changes Deploying an instance of VIAME-Web requires several configuration changes within your GCP project. These changes can be requested by submitting a System Change Request (SCR). To be able to access this web service, the SSH server's configuration (file: /etc/ssh/sshd_config) needs to include AllowTCPForwarding=yes. However, this is a baseline setting, specifically: \"CIS Benchmarks for Ubuntu Linux 20.04 LTS v1.1.0\" Server Level 2 - 5.3.20 \"Ensure SSH AllowTcpForwarding is disabled\". Thus this config change must be approved by NMFS Change Control Board (CCB) on an per-instance basis. The viame-web-noaa-gcp install scripts change this setting as necessary when the machines are being provisioned, and thus you only need to follow the deployment instructions once you have received approval to use this setting. If you are splitting services between a web and a worker node, you must allow communication between these VMs. In GCP networks are software defined, and thus all traffic is blocked unless a VPC firewall rule is created to allow it, even if it's on the same subnet. Thus, for split services, traffic between ports 8010 (Web traffic) and 5672 (RabbitMQ) within the subnet must be explicitly allowed between the web and worker nodes. In your SCR, request to have the network team add a tag that allows traffic between these ports for the IP addresses in your subnet. Apply this network tag to your web and worker VMs. Private Google Access must be enabled within the project to allow (along with appropriate service account permissions) communication between the VM and buckets. This should now be turned on by default for every project. You can confirm that Private Google Access is enabled for your subnet by following these instructions .","title":"Network Changes"},{"location":"network-changes/#noaa-fisheries-cloud-network-changes","text":"Deploying an instance of VIAME-Web requires several configuration changes within your GCP project. These changes can be requested by submitting a System Change Request (SCR). To be able to access this web service, the SSH server's configuration (file: /etc/ssh/sshd_config) needs to include AllowTCPForwarding=yes. However, this is a baseline setting, specifically: \"CIS Benchmarks for Ubuntu Linux 20.04 LTS v1.1.0\" Server Level 2 - 5.3.20 \"Ensure SSH AllowTcpForwarding is disabled\". Thus this config change must be approved by NMFS Change Control Board (CCB) on an per-instance basis. The viame-web-noaa-gcp install scripts change this setting as necessary when the machines are being provisioned, and thus you only need to follow the deployment instructions once you have received approval to use this setting. If you are splitting services between a web and a worker node, you must allow communication between these VMs. In GCP networks are software defined, and thus all traffic is blocked unless a VPC firewall rule is created to allow it, even if it's on the same subnet. Thus, for split services, traffic between ports 8010 (Web traffic) and 5672 (RabbitMQ) within the subnet must be explicitly allowed between the web and worker nodes. In your SCR, request to have the network team add a tag that allows traffic between these ports for the IP addresses in your subnet. Apply this network tag to your web and worker VMs. Private Google Access must be enabled within the project to allow (along with appropriate service account permissions) communication between the VM and buckets. This should now be turned on by default for every project. You can confirm that Private Google Access is enabled for your subnet by following these instructions .","title":"NOAA Fisheries Cloud Network Changes"},{"location":"support/","text":"Support For support regarding the use of VIAME-Web, see the User Guide and Frequently Asked Questions . For support with the deployment an instance of VIAME-Web in the NOAA Fisheries Cloud, there are several options: For issues with or clarification on the deployment instructions on this site, you can create an issue or contact Sam directly at sam.woodman@noaa.gov For issues specifically related to your Fisheries Cloud GCP project, contact Ed Rodgers at ed.rodgers@noaa.gov For general support you can also contact the VIAME-Web team directly at viame-web@kitware.com","title":"Support"},{"location":"support/#support","text":"For support regarding the use of VIAME-Web, see the User Guide and Frequently Asked Questions . For support with the deployment an instance of VIAME-Web in the NOAA Fisheries Cloud, there are several options: For issues with or clarification on the deployment instructions on this site, you can create an issue or contact Sam directly at sam.woodman@noaa.gov For issues specifically related to your Fisheries Cloud GCP project, contact Ed Rodgers at ed.rodgers@noaa.gov For general support you can also contact the VIAME-Web team directly at viame-web@kitware.com","title":"Support"}]}